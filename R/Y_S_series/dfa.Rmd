---
title: "DFA yellow and silver eels"
author: "ICES Data Group"
date: "18/08/2020"
output: html_document
---

```{r setup, include=FALSE}
load_library=function(necessary) {
	if(!all(necessary %in% installed.packages()[, 'Package']))
		install.packages(necessary[!necessary %in% installed.packages()[, 'Package']], dep = T)
	for(i in 1:length(necessary))
		library(necessary[i], character.only = TRUE)
}

knitr::opts_chunk$set(echo = TRUE)
username = Sys.info()[["user"]]
if (username=="hilaire.drouineau"){
  setwd("/home/hilaire.drouineau/Documents/Bordeaux/migrateurs/WGEEL/github/wg_WGEEL/R/Y_S_series/")
}
load_library("MARSS")
load_library("xtable")
load_library("getPass")
load_library("RPostgreSQL")
load_library("sf")
load_library("broom")
load_library("dplyr")
load_library("knitr")
load_library("ggplot2")
load_library("ggmap")
load_library("tidyverse")
load_library("rnaturalearth")
load_library("parallel")
con_wgeel=dbConnect(PostgreSQL(),
                    dbname="wgeel",
                    host="localhost",
                    port=5435,
	                  user= getPass(msg="username"),
	                  password= getPass())
CY<-2020 # current year ==> dont forget to update the graphics path below (

format_dfa =function(the.fit){
  H.inv=1
  Z.est = coef(the.fit, type="matrix")$Z
  if(ncol(Z.est)>1) H.inv = varimax(Z.est)$rotmat
  #rotate factor loadings
  Z<-coef(the.fit,type="matrix")$Z%*%H.inv
  #rotate trends
  trends<-solve(H.inv)%*%the.fit$states
  scale_val=coef(the.fit)$A
  list(Z=Z,trends=trends,scale_val=scale_alpha)
}
```

```{r data loading, include=FALSE}
query='SELECT 
		das_id,
		das_value,       
		das_year,
		das_comment,
		ser_id,            
		ser_order,
		ser_nameshort,
		ser_area_division,
		ser_qal_id,
    ser_hty_code,
    ser_emu_nameshort,
    ser_cou_code,
    ser_comment,
    ser_sam_id,
		das_qal_id,
		das_last_update,
		f_subarea,
		lfs_code,          
		lfs_name
		from datawg.t_dataseries_das 
		join datawg.t_series_ser on das_ser_id=ser_id
		left join ref.tr_lifestage_lfs on ser_lfs_code=lfs_code
		left join ref.tr_faoareas on ser_area_division=f_division
		where ser_typ_id in (2,3)'
mydata = dbGetQuery(con_wgeel,query)
mydata$ser_nameshort=as.factor(mydata$ser_nameshort)
mydata$ser_area_division=as.factor(mydata$ser_area_division)
mydata$f_subarea=as.factor(mydata$f_subarea)
mydata$lfs_code=as.factor(mydata$lfs_code)
mydata$ser_cou_code=as.factor(mydata$ser_cou_code)
mydata$ser_emu_nameshort=as.factor(mydata$ser_emu_nameshort)
mydata$lfs_name=as.factor(mydata$lfs_name)
mydata$ser_hty_code=as.factor(mydata$ser_hty_code)


silver_data <- mydata %>%
  filter(lfs_code=="S")
yellow_data <- mydata %>%
  filter(lfs_code=="Y")
yellow_map=st_read(con_wgeel,query=paste("select ser_id,geom from datawg.t_series_ser where ser_id in (", paste(unique(yellow_data$ser_id),collapse=","),")",sep=""))
silver_map=st_read(con_wgeel,query=paste("select ser_id,geom from datawg.t_series_ser where ser_id in (", paste(unique(silver_data$ser_id),collapse=","),")",sep=""))

dbDisconnect(con_wgeel)

```


# Introduction
Several time series of abundance of yellow eels and silver eels are collected throughout Europe. However, the analysis of their trends is more complex than for glass eel time-series since yellow and silver abundances are the results of both the general status of the population and local conditions (environmental condition and anthropogenic pressures) in river basins in which they are collected. Despite these difficulties, it would be interested to detect whether some common trends exist among the available data series and whether they can be related to monitoring methods, locations of the data or other factors. This would be a first exploration step before moving forward in a potential assessment of the standing stock.
In this context, we carry out a dynamic Factor Analysis, a multivariate method aiming at detecting common trends in a set of time-series (Zuur et al., 2003). Similary to glass-eel recruitment models, we restricted the data set to times series for which at least 10 years of data were available and carry out silver eel and yellow eel analysis separetely.

# Yellow eel
## Available data
`r length(unique(yellow_data$ser_id))` time series are available, originating from `r length(unique(yellow_data$ser_cou_code))` countries and `r length(unique(yellow_data$ser_emu_nameshort))` EMUs. Most of them are located in the Bristish Islands or France. `r length(unique(yellow_data$ser_id[yellow_data$ser_hty_code=="C"]))` time series are collected in coastal waters, `r length(unique(yellow_data$ser_id[yellow_data$ser_hty_code=="T"]))` in transitional waters and `r length(unique(yellow_data$ser_id[yellow_data$ser_hty_code=="F"]))` in freshwater.

```{r map yellow,echo=FALSE,message=FALSE,warning=FALSE}
worldmap <- ne_countries(scale = 'medium', type = 'map_units',
                         returnclass = 'sf')
europe_cropped <- st_crop(worldmap, xmin = -13, xmax = 25,
                                    ymin = 35, ymax = 65)
ggplot(yellow_map)+geom_sf(data=europe_cropped)+geom_sf(data=yellow_map,col="red")+
  theme_bw()
```

There are few data series started before 2000 so we sill restrict the analysis to the dataseries that have at least 10 observations after 2000. 

```{r pointperyearYE,echo=FALSE}
point_per_year_YE<- yellow_data %>%
  group_by(das_year) %>%
  summarize(n=n_distinct(ser_id))

ggplot(point_per_year_YE,aes(y=n,x=das_year))+
  geom_line()+
  xlab("")+ylab("number of available data")+
  theme_bw()

#we keep only data with at least 10 years
nbpoints = yellow_data %>%
  filter(das_year>=2000)%>%
  group_by(ser_id) %>%
  summarise(nbyear=n_distinct(das_year))%>%
  filter(nbyear>=10)
yellow_data <-yellow_data %>%
  filter(ser_id %in% nbpoints$ser_id & das_year>=2000)
```
This leaves `r length(unique(yellow_data$ser_id))` time series.

```{r plotTS YE,echo=FALSE,warning=FALSE}
yellow_data_std=yellow_data%>%
  mutate(das_value=log(das_value))%>%
  group_by(ser_id)%>%
  mutate(das_value=(das_value-mean(das_value,na.rm=TRUE))/sd(das_value,na.rm=TRUE))

ggplot(yellow_data_std,aes(x=das_year,y=das_value))+
  geom_line(aes(col=as.factor(ser_nameshort)))+
  geom_smooth(method="gam")+theme_bw()+xlab("year")+
  ylab("Standardised abundance index (log scale)")
```


## Running the DFA
The DFA method is fully detailed in Zuur 2003. The basic idea is to decompose each time series into a weighted sum of a few common trends and a noise factor:
$$
\begin{aligned}
Y_{j,t}=\mu_j + \sum_{i=1}^{n} w_{i,j} \cdot X_{i,t} +\epsilon_{j,t} \qquad \mbox{ with } \left \{ \epsilon_{j,t} \right \}  \sim N(0,\Sigma)
\end{aligned}
$$
with $Y_{j, t}$ the value of the series $j$ at time $t$, $\mu_j$ an intercept, $n the number of common trends, $w_{i, j}$ the weight of trend $i$ in the series $j$, $X_{i,t}$ the value of trend $i$ at time $t and $\epsilon_{j,t}$ a normal noise, potentially correlated between series through the variance covariance matrix $\Sigma$ . Therefore, $X_{i,t}$ represent the trends common to the series and are modelled as random walks:
$$
\begin{aligned}
X_{i,t}=X_{i,t-1}+f_{i,t} \qquad \mbox{ with } f_{i,t} \sim N(0,Q)
\end{aligned}
$$

with $f_{i,t} the noise on the trend $i$ at time $t$ which follows a normal law, possibly correlated between trends with the covariance covariance matrix $Q$ which can be set to the identity matrix (Zuur et al. 2003).
The method thus allows both to extract the common trends through the estimates of $X$, but also to see to what extent each of the importance of each trend in each series through $w$.

To fit the DFA, the user as to put some additionnal constraints. We will make 3 kind of assumptions on $\Sigma$:
* $\Sigma$ is a diagonal matrix with equal elements in the diagonal (e.g. time series are independent with similar values of noise)
* $\Sigma$ is a diagonal matrix with unequal elements in the diagonal (e.g. time series are independent with different values of noise)
* $\Sigma$ is a unconstrained (e.g. time series are potentially not independent with different values of noise) (this solution was not tested for Yellow eel since the number of time series was too large compared to the number of observations)
1 to 4 common trends are tested. The best combination of $\Sigma$ and number of trends is chosen by comparing AICc criterion. 


## Common trends
Before running the DFA, values were logtransformed (a 0 value was recorded for a single observation (time series SkaY) and was replaced by 10% of its lower value) and scales ((mean deleted and divided by the standard deviation)).

```{r dfa yellow, results='hide',eval=FALSE}
#replace 0 value
yellow_data$das_value[yellow_data$das_value==0 & yellow_data$ser_nameshort=="SkaY"] <- 0.1 * min(yellow_data$das_value[yellow_data$das_value>0 & yellow_data$ser_nameshort=="SkaY"],na.rm=TRUE)

#put data in wide format and select from 1980
yellow_wide <-yellow_data %>%
  mutate(das_value=log(das_value))%>%
  select(ser_nameshort,das_value,das_year) %>%
  arrange(das_year)%>%
  pivot_wider(id_cols=c(ser_nameshort,das_year),names_from=das_year,values_from=das_value)

#then we scaled time series (substract the mean and divide by standard deviation)
yellow_wide[,-1] <- sweep(yellow_wide[,-1],1,rowMeans(yellow_wide[,-1],na.rm=TRUE),"-")
yellow_wide[,-1] <- sweep(yellow_wide[,-1],1,apply(yellow_wide[,-1],1,sd,na.rm=TRUE),"/")


#Define some constants
N_ts=nrow(yellow_wide) #number of time series
TT=ncol(yellow_wide)-1 #number of time steps
y=as.matrix(yellow_wide[,-1]) #matrix of obseravation
rownames(y)=yellow_wide$ser_nameshort

#Design of experiments
#S=c("diagonal and equal","diagonal and unequal","unconstrained")
S=c("diagonal and equal","diagonal and unequal")
nbtrend=1:4
expe=expand.grid(S=S,nbtrend=nbtrend)

#Now we make a loop of DFA to find the best model
model_comparisonsYE= mcmapply(function(s,m){
  dfa.model=list(R=s, m=m)
  kemz=MARSS(y, model=dfa.model, form="dfa",z.score=TRUE,control=list(maxit=2000))
  aicc=MARSSaic(kemz, output="AICc")$AICc
  aic=MARSSaic(kemz, output="AIC")$AIC
  list(Trends=m,AICc=aicc,AIC=aic,Sigma=s,dfa=kemz)
},as.character(expe$S),expe$nbtrend, mc.cores=4,SIMPLIFY=FALSE)

  results_yellow=do.call(rbind.data.frame,lapply(model_comparisonsYE,function(x) {
  data.frame(Trends=x["Trends"],
             Sigma=x["Sigma"],
             AIC=x["AIC"],
             AICc=x["AICc"])})
)
save.image(file="./dfa.rdata")
```

```{r comparisonYE,echo=FALSE}
load("./dfa.rdata")
kable(results_yellow,digits=2)
best_fit_yellow=model_comparisonsYE[[which(results_yellow$AIC==min(results_yellow$AIC))]]$dfa
```


The factor loadings are displayed in the following plots (importance of each trend in each time series)

```{r series loadings yellow, echo=FALSE}
N_ts=nrow(yellow_wide) #number of time series
TT=ncol(yellow_wide)-1 #number of time steps
y=as.matrix(yellow_wide[,-1]) #matrix of obseravation
rownames(y)=yellow_wide$ser_nameshort

formatted_matrices_yellow=format_dfa(best_fit_yellow)

namesseries=yellow_wide$ser_nameshort

trends=formatted_matrices_yellow$trends
Z=formatted_matrices_yellow$Z
#plot the factor loadings
minZ = 0.15
m=dim(trends)[1]
ylims = c(-1.1*max(abs(Z)), 1.1*max(abs(Z)))
par(mfrow=c(ceiling(m/2),2), mar=c(0.5,2.5,1.5,0.5), oma=c(0.4,1,1,1))
for(i in 1:m) {
    #plot(c(1:N_ts)[abs(Z[,i])>minZ], as.vector(Z[abs(Z[,i])>minZ,i]),
  #     type="h", lwd=2, xlab="", ylab="", xaxt="n", ylim=ylims, xlim=c(0,N_ts+1))
  plot(c(1:N_ts), as.vector(Z[,i]),
       type="h", lwd=2, xlab="", ylab="", xaxt="n", ylim=ylims, xlim=c(0,N_ts+1))
  for(j in 1:N_ts) {
    # if(Z[j,i] > minZ) {text(j, -0.05, namesseries[j], srt=90, adj=1, cex=0.5)}
    # if(Z[j,i] < -minZ) {text(j, 0.05, namesseries[j], srt=90, adj=0, cex=0.5)}
    if(Z[j,i] > 0) {text(j, -0.05, namesseries[j], srt=90, adj=1, cex=0.5,col=ifelse(Z[j,i]>minZ,2,1))}
    if(Z[j,i] < 0) {text(j, 0.05, namesseries[j], srt=90, adj=0, cex=0.5,col=ifelse(Z[j,i]< -minZ,2,1))}
    abline(h=0, lwd=1, col="gray")
  } # end j loop
  mtext(bquote(~italic(z[list(i,.(i))])),side=3,line=.5)
} # end i loop

##kept if two trends
loadings_y = as.data.frame(Z[,1:2])
names(loadings_y)=c("loading_trend_1","loading_trend_2")
ggplot(loadings_y,aes(x=loading_trend_1,y=loading_trend_2))+
  geom_point()+
  theme_bw()
```


```{r trends yellow, echo=FALSE}
trends_long <- as.data.frame(t(trends))
names(trends_long)=paste("Trend",1:ncol(trends_long))
trends_long$year=as.numeric(names(yellow_wide)[-1])
trends_long<-trends_long %>%
  pivot_longer(starts_with("Trend"),names_to="Trend")
ggplot(trends_long,aes(x=year,y=value))+
  geom_line()+
  facet_wrap(.~Trend)+
  theme_bw()

```



```{r fits yellow, echo=FALSE,warning=FALSE}
d <- broom::augment(best_fit_yellow,interval="confidence")
ggplot(data = d) +
  geom_line(aes(t, .fitted)) +
  geom_point(aes(t, y)) +
  geom_ribbon(aes(x=t, ymin=.conf.low, ymax=.conf.up), linetype=2, alpha=0.2) +
  facet_wrap(vars(.rownames)) +
  xlab("") + ylab("standarized abundance index")+
  scale_x_continuous(breaks=seq(1,TT,10),labels=colnames(y)[seq(1,TT,10)])+
  theme_bw()
```

# Silver eel
## Available data
`r length(unique(silver_data$ser_id))` time series are available, originating from `r length(unique(silver_data$ser_cou_code))` countries and `r length(unique(silver_data$ser_emu_nameshort))` EMUs. Most of them are located in Northern Europe. `r length(unique(silver_data$ser_id[silver_data$ser_hty_code=="C"]))` time series are collected in coastal waters and `r length(unique(silver_data$ser_id[silver_data$ser_hty_code=="F"]))` in freshwater, while the habitat type was not reported for .`r length(unique(silver_data$ser_id[is.na(silver_data$ser_hty_code)]))`

```{r map silver,echo=FALSE}
g=ggplot(silver_map)+geom_sf(data=europe_cropped)+geom_sf(data=silver_map,col="red")+
  theme_bw()
print(g)
```

There are few data series started before 2000 so we sill restrict the analysis to the dataseries that have at least 10 observations after 2000. 

```{r pointperyearSE,echo=FALSE}
point_per_year_SE<- silver_data %>%
  group_by(das_year) %>%
  summarize(n=n_distinct(ser_id))

ggplot(point_per_year_SE,aes(y=n,x=das_year))+
  geom_line()+
  xlab("")+ylab("number of available data")+
  theme_bw()

#we keep only data with at least 10 years
nbpoints = silver_data %>%
  filter(das_year>=2000)%>%
  group_by(ser_id) %>%
  summarise(nbyear=n_distinct(das_year))%>%
  filter(nbyear>=10)
silver_data <-silver_data %>%
  filter(ser_id %in% nbpoints$ser_id & das_year>=2000)
```

This leaves `r length(unique(silver_data$ser_id))` time series.


```{r plotTS SE,echo=FALSE,warnings=FALSE}
silver_data_std=silver_data%>%
  mutate(das_value=log(das_value))%>%
  group_by(ser_id)%>%
  mutate(das_value=(das_value-mean(das_value,na.rm=TRUE))/sd(das_value,na.rm=TRUE))

ggplot(silver_data_std,aes(x=das_year,y=das_value))+
  geom_line(aes(col=as.factor(ser_nameshort)))+
  geom_smooth(method="gam")+theme_bw()+xlab("year")+
  ylab("Standardised abundance index (log scale)")
```
## Running the DFA
We applied the same method as for yellow eel so readers can refer to the corresponding section for further details.

## Common trends
Before running the DFA, values were logtransformed (a 0 value was recorded for a single observation (time series SkaY) and was replaced by 10% of its lower value) and scales ((mean deleted and divided by the standard deviation)).

```{r dfa silver, results='hide',eval=FALSE}
N_ts=nrow(silver_wide) #number of time series
TT=ncol(silver_wide)-1 #number of time steps
y=as.matrix(silver_wide[,-1]) #matrix of obseravation
rownames(y)=silver_wide$ser_nameshort

#put data in wide format and select from 1980
silver_wide <-silver_data %>%
  mutate(das_value=log(das_value))%>%
  select(ser_nameshort,das_value,das_year) %>%
  arrange(das_year)%>%
  pivot_wider(id_cols=c(ser_nameshort,das_year),names_from=das_year,values_from=das_value)

#then we scaled time series (substract the mean and divide by standard deviation)
silver_wide[,-1] <- sweep(silver_wide[,-1],1,rowMeans(silver_wide[,-1],na.rm=TRUE),"-")
silver_wide[,-1] <- sweep(silver_wide[,-1],1,apply(silver_wide[,-1],1,sd,na.rm=TRUE),"/")


#Define some constants
N_ts=nrow(silver_wide) #number of time series
TT=ncol(silver_wide)-1 #number of time steps
y=as.matrix(silver_wide[,-1]) #matrix of obseravation
rownames(y)=silver_wide$ser_nameshort

#Design of experiments
S=c("diagonal and equal","diagonal and unequal","unconstrained")
#S=c("diagonal and equal","diagonal and unequal")
nbtrend=1:4
expe=expand.grid(S=S,nbtrend=nbtrend)

#Now we make a loop of DFA to find the best model
model_comparisonsSE= mcmapply(function(s,m){
  dfa.model=list(R=s, m=m)
  kemz=MARSS(y, model=dfa.model, form="dfa",z.score=TRUE,control=list(maxit=2000))
  if (kemz$convergence>0) kemz=MARSS(y, model=dfa.model, form="dfa",z.score=TRUE,control=list(maxit=2000),method="BFGS")
  aicc=MARSSaic(kemz, output="AICc")$AICc
  aic=MARSSaic(kemz, output="AIC")$AIC
  list(Trends=m,AICc=aicc,AIC=aic,Sigma=s,dfa=kemz,convergence=kemz$convergence==0)
},as.character(expe$S),expe$nbtrend, mc.cores=4,SIMPLIFY=FALSE)

  results_silver=do.call(rbind.data.frame,lapply(model_comparisonsSE,function(x) {
  data.frame(Trends=x["Trends"],
             Sigma=x["Sigma"],
             AIC=x["AIC"],
             AICc=x["AICc"],
             convergence=x["convergence"])})
)
save.image(file="./dfa.rdata")
```

```{r comparisonSE,echo=FALSE}
load("./dfa.rdata")
kable(results_silver,digits=2)
best_fit_silver=model_comparisonsSE[[which(results_silver$AIC==min(results_silver$AIC))]]$dfa
```


The factor loadings are displayed in the following plots (importance of each trend in each time series)

```{r series loadings silver, echo=FALSE}
N_ts=nrow(silver_wide) #number of time series
TT=ncol(silver_wide)-1 #number of time steps
y=as.matrix(silver_wide[,-1]) #matrix of obseravation
rownames(y)=silver_wide$ser_nameshort

formatted_matrices_silver=format_dfa(best_fit_silver)

namesseries=silver_wide$ser_nameshort

trends=formatted_matrices_silver$trends
Z=formatted_matrices_silver$Z
#plot the factor loadings
minZ = 0.15
m=dim(trends)[1]
ylims = c(-1.1*max(abs(Z)), 1.1*max(abs(Z)))
par(mfrow=c(ceiling(m/2),2), mar=c(0.5,2.5,1.5,0.5), oma=c(0.4,1,1,1))
for(i in 1:m) {
    #plot(c(1:N_ts)[abs(Z[,i])>minZ], as.vector(Z[abs(Z[,i])>minZ,i]),
  #     type="h", lwd=2, xlab="", ylab="", xaxt="n", ylim=ylims, xlim=c(0,N_ts+1))
  plot(c(1:N_ts), as.vector(Z[,i]),
       type="h", lwd=2, xlab="", ylab="", xaxt="n", ylim=ylims, xlim=c(0,N_ts+1))
  for(j in 1:N_ts) {
    # if(Z[j,i] > minZ) {text(j, -0.05, namesseries[j], srt=90, adj=1, cex=0.5)}
    # if(Z[j,i] < -minZ) {text(j, 0.05, namesseries[j], srt=90, adj=0, cex=0.5)}
    if(Z[j,i] > 0) {text(j, -0.05, namesseries[j], srt=90, adj=1, cex=0.5,col=ifelse(Z[j,i]>minZ,2,1))}
    if(Z[j,i] < 0) {text(j, 0.05, namesseries[j], srt=90, adj=0, cex=0.5,col=ifelse(Z[j,i]< -minZ,2,1))}
    abline(h=0, lwd=1, col="gray")
  } # end j loop
  mtext(bquote(~italic(z[list(i,.(i))])),side=3,line=.5)
} # end i loop

##kept if two trends
# loadings_y = as.data.frame(Z[,1:2])
# names(loadings_y)=c("loading_trend_1","loading_trend_2")
# ggplot(loadings_y,aes(x=loading_trend_1,y=loading_trend_2))+
#   geom_point()+
#   theme_bw()
```


```{r trends silver, echo=FALSE}
trends_long <- as.data.frame(t(trends))
names(trends_long)=paste("Trend",1:ncol(trends_long))
trends_long$year=as.numeric(names(silver_wide)[-1])
trends_long<-trends_long %>%
  pivot_longer(starts_with("Trend"),names_to="Trend")
ggplot(trends_long,aes(x=year,y=value))+
  geom_line()+
  facet_wrap(.~Trend)+
  theme_bw()

```



```{r fits silver, echo=FALSE,warning=FALSE}
d <- broom::augment(best_fit_silver,interval="confidence")
ggplot(data = d) +
  geom_line(aes(t, .fitted)) +
  geom_point(aes(t, y)) +
  geom_ribbon(aes(x=t, ymin=.conf.low, ymax=.conf.up), linetype=2, alpha=0.2) +
  facet_wrap(vars(.rownames)) +
  xlab("") + ylab("standarized abundance index")+
  scale_x_continuous(breaks=seq(1,TT,10),labels=colnames(y)[seq(1,TT,10)])+
  theme_bw()
```
