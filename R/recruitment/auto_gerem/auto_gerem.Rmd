---
title: "GEREM"
author: "ICES Data Group"
date: "08/09/2020"
documentclass: article
output: 
  bookdown::html_document2:
    fig_caption: yes
    number_sections: yes
bibliography: gerem.bib
params:
  updateData: FALSE
  runModel:   FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message=FALSE)
thin=1
load_library=function(necessary) {
	if(!all(necessary %in% installed.packages()[, 'Package']))
		install.packages(
		  necessary[!necessary %in% installed.packages()[, 'Package']], dep = T)
	for(i in 1:length(necessary))
		library(necessary[i], character.only = TRUE)
}

CY<-2020 # current year ==> dont forget to update the graphics path below (

load_library("bookdown")
load_library("coda")
load_library("runjags")
load_library("getPass")
load_library("RPostgres")
load_library("sf")
load_library("dplyr")
load_library("ggplot2")
load_library("parallel")
load_library("tidyr")
load_library("reshape2")
load_library("rnaturalearth")
knitr::opts_chunk$set(echo = TRUE)
username = Sys.info()[["user"]]

updatedData = FALSE
if ((!file.exists(paste("datagerem",CY,".rdata",sep=""))) | params$updateData){
  if (username=="hilaire.drouineau"){
    setwd("~/Documents/Bordeaux/migrateurs/WGEEL/github/wg_WGEEL/R/recruitment/auto_gerem/")
    con_ccm=dbConnect(Postgres(),host="citerne.bordeaux.irstea.priv",dbname="referentiel",
                      user="hilaire.drouineau",
                      password=getPass("Password for ccm database"))
    table_seaoutlets="hydrographie.ccm_v2_1_riverbasin_seaoutlets"
  }
  
  #get the surface of a catchment given a list of wso_id
  getSurface=function(w,con){
    if (is.na(w)) return(1) #corresponds to unreal catchments
    w <- gsub("\\{","(",w)
    w <- gsub("\\}",")",w)
    dbGetQuery(con,paste("select sum(area_km2) from",
                         table_seaoutlets,
                         "where wso_id in ",w))[1,1]
  }
  
  con_wgeel=dbConnect(Postgres(),
                      dbname="wgeel",
                      host="localhost",
                      port=5435,
  	                  user= getPass(msg="username"),
  	                  password= getPass())
  updatedData = TRUE

} else {
  load(paste("datagerem",CY,".rdata",sep=""))
}
```

# Introduction
GEREM is a Bayesian model aiming at estimating glass eel recruitment at different nested spatial scales (overall recruitment, sub-regions/zone, river basins) through the analysis of available recruitment time-series [@drouineau2016]. The model has already applied in France [@drouineau2016], to a large part of Europe [@bornarel2018a] and is currently used in the Sudoang Interreg projet. The model assumes that each year, the overall recruitment $R(y)$ is distributed among various zones (i.e. subregions) which receive recruitment $R_z(y)$. Then, zone recruitment is distributed among river catchments as a function of their surface, leading to recruitment $R_{c,z}(y)$. Basically, GEREM is a mixiong of a Dynamic Factor Analysis (DFA) [@zuur2003a] and a “rule of three”. Similarly to a DFA model, GEREM is state-space model based on a random walk structure, that estimates common trends in a set of time-series. The rule of three is used to extrapolate absolute recruitment estimates in a river basin to recruitment in other basins in the same zone, stating that the recruitment in each basin is a simple function of its surface. After having inventoried available time series and listed their characteristics, it is necessary to define zones. In each zone: * river catchements should have similar trends in recruitment * the rule of three must apply, i.e. it should be possible to extrapolate recruitment in a basin to another basin of the same zone as a simple function of their relative surfaces * time series of recruitment should be available. Morevover, there should be at least on time series of absolute recruitment.

The model is detailed in [@drouineau2016] and [@bornarel2018a]. The current exercice is mainly an update from [@bornarel2018a]. We will use the same zone and the nearly the same time series but with updated values.


# Material and Methods

```{r dataLoadingFormatting, echo=FALSE, warning=FALSE, message=FALSE}
####loading time series from WGEEL
if ((!file.exists(paste("datagerem",CY,".rdata",sep=""))) | params$updateData){
  
  series_wgeel=read.table("catchment_wgeel.csv",header=TRUE,sep=";")
  wgeel=dbGetQuery(con_wgeel,paste("select das_year,ser_nameshort,ser_uni_code,das_value,ser_ccm_wso_id,ser_x,ser_y from datawg.t_dataseries_das left join datawg.t_series_ser on das_ser_id =ser_id where das_year>=1960 and ser_nameshort in ('",paste(series_wgeel$ser_nameshort[series_wgeel$from_wgeel],collapse="','"),"')",sep="") )
  series_wgeel <- merge(series_wgeel,
                        unique(wgeel[,c("ser_nameshort","ser_ccm_wso_id","ser_x","ser_y")]),
                        all.x=TRUE)
  series_wgeel$ser_ccm_wso_id[series_wgeel$ser_nameshort=="MinG"] <-
    series_wgeel$ser_ccm_wso_id[series_wgeel$ser_nameshort=="MiSpG"]
  series_wgeel$ser_ccm_wso_id[!series_wgeel$from_wgeel] = 
    as.character(series_wgeel$wso_id[!series_wgeel$from_wgeel])
  series_wgeel$surface=sapply(series_wgeel$ser_ccm_wso_id,getSurface,con=con_ccm )
  
  
  ###converting to kg
  wgeel$das_value=ifelse(wgeel$ser_uni_code=="t",
                         wgeel$das_value*1000,
                         ifelse(wgeel$ser_uni_code=="nr",
                                wgeel$das_value*0.3/1000,
                                wgeel$das_value))
  
  ###reshaping and merging Minho Spain and Portugal
  wgeel_wide=wgeel %>%
    select(das_year,das_value,ser_nameshort) %>%
    pivot_wider(names_from=ser_nameshort,values_from=das_value,id_cols=das_year)
  wgeel_wide$MinG=wgeel_wide$MiPoG+wgeel_wide$MiSpG
  series_wgeel[series_wgeel$ser_nameshort=="MinG",c("ser_x","ser_y")]=
    series_wgeel[series_wgeel$ser_nameshort=="MiSpG",c("ser_x","ser_y")]
  wgeel_wide <- wgeel_wide %>%
    select(-MiPoG,-MiSpG)
  series_wgeel <- series_wgeel %>%
    filter(!ser_nameshort %in% c("MiSpG","MiPoG"))
  
    
  
  
  ####loading additional french series
  french_wide=read.table("french_serie2.csv",header=TRUE,sep=";")
  names(french_wide)[1]="das_year"
  french_wide=subset(french_wide,french_wide$das_year>=1960)
  

  tmp <- unique(na.omit(series_wgeel[,c("ser_x","ser_y","ser_ccm_wso_id")]))
  series_wgeel$ser_x[!series_wgeel$from_wgeel]=
    tmp$ser_x[match(series_wgeel$ser_ccm_wso_id[!series_wgeel$from_wgeel],
                             tmp$ser_ccm_wso_id)]
  series_wgeel$ser_y[!series_wgeel$from_wgeel]=
    tmp$ser_y[match(series_wgeel$ser_ccm_wso_id[!series_wgeel$from_wgeel],
                             tmp$ser_ccm_wso_id)]
                
  series_wgeel[series_wgeel$ser_nameshort=="SeGEMAC",c("ser_x","ser_y")] =
    c(-1.136938,45.796857)
  series_wgeel[series_wgeel$ser_nameshort=="ChGEMAC",c("ser_x","ser_y")] =
    c(-1.076013,45.953153)
  series_wgeel[series_wgeel$ser_nameshort=="Somme",c("ser_x","ser_y")] =
    c(1.644597,50.181853)
  series_wgeel[series_wgeel$ser_nameshort=="Oria",c("ser_x","ser_y")] =
    c(-2.1307297,43.2827)
  
  
  values_wide=merge(wgeel_wide,french_wide,all=TRUE)
  series=series_wgeel
  
  values_long <- values_wide %>%
    pivot_longer(-das_year,
                 names_to="ser_nameshort",
                 values_to="das_value")

  series_stat <- merge(series, values_long) %>%
    select(-wso_id,-ser_ccm_wso_id,-scale_bound_shape2,-scale_bound_shape1) %>%
    filter(!is.na(das_value)) %>%
    group_by(ser_nameshort, type, zone, surface) %>%
    summarize(first_year=min(das_year),
              last_year=max(das_year),
              nbyear=n_distinct(das_year))
  values_long <- na.omit(merge(values_long,
                               series[,c("ser_nameshort","zone")]))
  data_points <- values_long %>%
    group_by(zone,das_year) %>%
    summarise(n=n(),pre=n()>0)
}
```

```{r buildingZone, echo=FALSE, warning=FALSE, message=FALSE}
######building zones
if ((!file.exists(paste("datagerem",CY,".rdata",sep=""))) | params$updateData){
  
  allcatchments=st_read(con_ccm,query=paste('select wso_id ser_ccm_wso_id,area_km2,"window",sea_cd,geom from',table_seaoutlets,' where strahler>0 and ("window"<=2004 or "window"=2008)'))
  outlets=st_read(con_ccm,query='select "window",gid, wso_id ser_ccm_wso_id, geom from hydrographie.ccm_v2_1_w2000_rivernodes where num_seg=0 union
                  select "window",gid, wso_id ser_ccm_wso_id, geom from hydrographie.ccm_v2_1_w2001_rivernodes where num_seg=0 union
                  select "window",gid, wso_id ser_ccm_wso_id, geom from hydrographie.ccm_v2_1_w2002_rivernodes where num_seg=0 union
                  select "window",gid, wso_id ser_ccm_wso_id, geom from hydrographie.ccm_v2_1_w2003_rivernodes where num_seg=0 union
                  select "window",gid, wso_id ser_ccm_wso_id, geom from hydrographie.ccm_v2_1_w2004_rivernodes where num_seg=0 union
                  select "window",gid, wso_id ser_ccm_wso_id, geom from hydrographie.ccm_v2_1_w2008_rivernodes where num_seg=0')
  outlets=subset(outlets,outlets$ser_ccm_wso_id %in% allcatchments$ser_ccm_wso_id)
  
  emu=st_read(con_wgeel,query='select * from ref.tr_emu_emu')
  asso=st_nearest_feature(st_transform(outlets,4326),emu)
  outlets$emu=emu$emu_nameshort[asso]
  allcatchments$emu=outlets$emu[match(allcatchments$ser_ccm_wso_id,outlets$ser_ccm_wso_id)]
  
  allcatchments$zone=NA
  allcatchments$zone[startsWith(allcatchments$emu,"FR_") & allcatchments$sea_cd==1]="ATL_F"
  allcatchments$zone[(startsWith(allcatchments$emu,"ES_") | startsWith(allcatchments$emu,"PT_")) & allcatchments$sea_cd==1]="ATL_IB"
  allcatchments$zone[(allcatchments$sea_cd==4 & allcatchments$window!=2002) & (startsWith(allcatchments$emu,'FR_') | allcatchments$emu %in% c("GB_Wale","GB_Seve","GB_SouW","GB_SouE"))]="Channel"
  allcatchments$zone[allcatchments$sea_cd==5]="NS"
  allcatchments$zone[allcatchments$sea_cd!=5 &  ((startsWith(allcatchments$emu,'GB_') | startsWith(allcatchments$emu,'IE_')) &  !allcatchments$emu %in% c("GB_Wale","GB_Seve","GB_SouW","GB_SouE"))]="BI"
  allcatchments$zone[(startsWith(allcatchments$emu,"ES_") | startsWith(allcatchments$emu,"FR_")| startsWith(allcatchments$emu,"IT_")) & allcatchments$sea_cd==2]="Med"
  
  allcatchments=subset(allcatchments,!is.na(allcatchments$zone))
  allcatchments$zone=as.factor(allcatchments$zone)
  
  zone=aggregate(allcatchments$area_km2,list(allcatchments$zone),sum)
  names(zone)=c("zone","surface")
}
```


## Zone definition

We used the same zones as @bornarel2018a \@ref(fig:mapsZones):

* a North Sea zone (NS)
* a Channel zone which covers Southwestern Great Britanny and NorthWestern France
* ATL_F which covers the French coast along the Bay of Biscay
* ATL_IB which extends from the Cantabrian Sea to the the Gibraltar strait
* Med which extends from the Gibraltar strait to Sicilia

```{r mapsZones, fig.cap="Zone definition and available data", echo=FALSE,warning=FALSE,message=FALSE}
worldmap <- ne_countries(scale = 'medium', type = 'map_units',
                         returnclass = 'sf')
europe_cropped <- st_crop(worldmap, xmin = -13, xmax = 27,
                                    ymin = 35, ymax = 65)
ggplot(europe_cropped)+geom_sf(data=europe_cropped)+
  geom_point(data=series,aes(x=jitter(ser_x,amount=.5),
                             y=jitter(ser_y,amount=.5),
                             pch=type,
                             col=type))+
  geom_sf(data=allcatchments,aes(fill=zone),alpha=.3,col=NA)+
  scale_fill_viridis_d()+
  xlab("")+
  ylab("")+
  theme_bw()
```

## Available Data
Table \@ref(tab:availableData) summarizes the data use to fit the model. While time series are available in all zones, most absolute estimates come from ATL_F. In other zones, trap monitoring and commercial catches can inform on absolute estimates given but this requires making assumption on trapping efficiency or on exploitation rate. We also note the the number of time series is limited in the Channel area. Conversely, there are many time series in ATL_F, but most of them ended after the implementation of the French Eel Management Plan [@ministeredelecologiedelenergiedudeveloppementdurableetdelamenagementduterritoire2010] and presently, there is only one still updated time series. We also note that the Mediterranean zone is large with only four available time series.

```{r availableData, echo=FALSE}
knitr::kable(series_stat[order(series_stat$zone,series_stat$ser_nameshort),],
             caption="Availabe time-series of recruitment",
            col.names=c("Series","Type","Zone",
                        "Surface (km²)","First Year",
                        "Last Year","Nb data"))
```

For traps, we use vague priors on trap efficiencies to give an insight on the possible recruitment (Figure \@ref(fig:priorsScaling)): we used a prior around 1/3, value often estimated for trap efficiency [@briand2005; @drouineau2015; @jessop2000; @noonan2012]. Commercial catches time series were used as relative time series, except in the Somme river. In this river, the presence of an obstacle in the estuary leads to high catch rates. Therefore, following @bornarel2018a, we assumed that the exploitation rate was around 0.75 in this basin.

```{r priorsScaling, fig.cap="Priors for exploitation rates and trap efficiencies", echo=FALSE, warning=FALSE, message=FALSE}
priors=na.omit(unique(series[,c("type","scale_bound_shape1","scale_bound_shape2")]))
priors=expand_grid(priors,x=seq(0,1,.01))
priors <- priors %>%
  mutate(prior=dbeta(x,scale_bound_shape1,scale_bound_shape2))
priors$series=mapply(function(t,s1,s2) 
  paste(series$ser_nameshort[series$type==t &
                               series$scale_bound_shape1==s1 &
                               series$scale_bound_shape2==s2],
        collapse="\n"),
  priors$type,priors$scale_bound_shape1,priors$scale_bound_shape2)
ggplot(priors,aes(x=x, y=prior))+
  geom_line(aes(col=series))+
  xlab("scaling factor") + ylab("prior density")+
  scale_color_brewer("factor",type="qual")+
  theme_bw()
```

## Running the model
```{r runningConfig, echo=FALSE, include=FALSE}
if ((!file.exists(paste("datagerem",CY,".rdata",sep=""))) | params$updateData){
  
  burnin=100000
  sample=50000
  
  tmp=matrix(0,max(table(allcatchments$zone)),length(table(allcatchments$zone)))
  tab=table(allcatchments$zone)
  for (i in 1:length(tab)){
    tmp[1:tab[i],i]=allcatchments$area_km2[allcatchments$zone==names(tab)[i]]
    
  }
  colnames(tmp)=names(tab)
  tmp=tmp[,match(zone$zone,colnames(tmp))]
  surfaceallcatchment=t(tmp)
  
  
  
  
  
  
  
  row.names(values_wide)=values_wide$das_year
  values_wide=values_wide[,-1]
  
  
  ###############formatting data and inputs
  
  
  nbyear=nrow(values_wide)
  absolute=subset(values_wide,select=which(series$type[match(names(values_wide),series$ser_nameshort)]=="absolute"))
  serie=subset(values_wide,select=which(series$type[match(names(values_wide),series$ser_nameshort)]=="relative"))
  trap=subset(values_wide,select=which(series$type[match(names(values_wide),series$ser_nameshort)]=="trap"))
  catch=subset(values_wide,select=which(series$type[match(names(values_wide),series$ser_nameshort)]=="catch"))
  
  
  #serie=serie+1
  serie=sweep(serie,2, colMeans(serie,na.rm=TRUE),"/")
  logIAObs=as.matrix(log(serie))
  logIAObs[is.infinite(logIAObs)]=NA #we removed 0 
  logUObs=log(absolute)
  logIPObs=as.matrix(log(trap))
  logIPObs[is.infinite(logIPObs)]=NA #we removed 0 
  logIEObs=log(catch)
  
  nbsurvey=ncol(serie)
  nbabsolute=ncol(absolute)
  nbtrap=ncol(trap)
  nbcatch=ncol(catch)
  
  ########formatting catchments
  nbzone=nrow(zone)
  tab_series=unique(series[,c("ser_ccm_wso_id","surface","zone")]) # a table with one row per catchment in which we have data
  nbcatchments=nrow(tab_series)
  
  surface=tab_series$surface #vector of surfaces of the catchments
  zonecatchment=match(tab_series$zone,zone$zone)
  
  surfaceZone=zone$surface
  
  
  ###############creating vector of indices to match the different dataset
  catchment_survey=match(series$ser_ccm_wso_id[match(names(serie),series$ser_nameshort)],tab_series$ser_ccm_wso_id)
  catchment_absolute=match(series$ser_ccm_wso_id[match(names(absolute),series$ser_nameshort)],tab_series$ser_ccm_wso_id)
  catchment_trap=match(series$ser_ccm_wso_id[match(names(trap),series$ser_nameshort)],tab_series$ser_ccm_wso_id)
  catchment_catch=match(series$ser_ccm_wso_id[match(names(catch),series$ser_nameshort)],tab_series$ser_ccm_wso_id)
  
  meanlogq=rep(log(.5),ncol(serie))
  
  
  
  mulogRglobal1=log(sum(colMeans(absolute,na.rm=TRUE)))+log(sum(surfaceZone)/sum(surface[catchment_absolute]))
  
  initpropR=rep(1/nbzone,nbzone)

  
  mydata=list(
    initpropR=initpropR,
    nbzone=nbzone,
    nbsurvey=nbsurvey,
    nbtrap=nbtrap,
    nbcatchments=nbcatchments,
    nbabsolute=nbabsolute,
    nbcatch=nbcatch,
    catchment_survey=catchment_survey,
    catchment_trap=catchment_trap,
    catchment_catch=catchment_catch,
    catchment_absolute=catchment_absolute,
    zonecatchment=zonecatchment,
    surface=ifelse(!is.na(surface),surface,1),
    nbyear=nbyear,
    logIAObs=as.matrix(logIAObs),
    logIPObs=as.matrix(logIPObs),
    logUObs=as.matrix(logUObs),
    logIEObs=as.matrix(logIEObs),
    surfaceallcatchment=surfaceallcatchment,
    scale_trap=sapply(1:nbtrap,function(i){
        series[series$ser_nameshort==names(trap)[i],
               c("scale_bound_shape1",
                 "scale_bound_shape2")]
      }),
    scale_catch=sapply(1:nbcatch,function(i){
      series[series$ser_nameshort==names(catch)[i],
               c("scale_bound_shape1",
                 "scale_bound_shape2")]
    })
  )
  
  generate_init=function(){
    gen_init=function(x){
      epsilonRcm=rnorm(mydata$nbcatchments*mydata$nbyear)
      logIAObs=apply(mydata$logIAObs,2,function(x){
        d=which(!is.na(x))
        x[which(is.na(x))]=runif(length(which(is.na(x))),min(x,na.rm = TRUE)*2,max(x,na.rm = TRUE)*2)
        x[d]=NA
        x
      })
      logIPObs=apply(mydata$logIPObs,2,function(x){
        d=which(!is.na(x))
        x[which(is.na(x))]=runif(length(which(is.na(x))),min(x,na.rm = TRUE)*2,max(x,na.rm = TRUE)*2)
        x[d]=NA
        x
      })
      
      logUObs=apply(mydata$logUObs,2,function(x){
        d=which(!is.na(x))
        x[which(is.na(x))]=runif(length(which(is.na(x))),min(x,na.rm = TRUE)*2,max(x,na.rm = TRUE)*2)
        x[d]=NA
        x
      })
      logIEObs=apply(mydata$logIEObs,2,function(x){
        d=which(!is.na(x))
        x[which(is.na(x))]=runif(length(which(is.na(x))),min(x,na.rm = TRUE)*2,max(x,na.rm = TRUE)*2)
        x[d]=NA
        x
      })
      
      
      #inits
      propR=matrix(0,nbzone,nbyear)
      for (i in 1:nbyear){
        tmp=rbeta(mydata$nbzone,1,1)
        propR[,i]=tmp/sum(tmp)
      }
      
      tauq=1/(runif(1,0.26,1)^2)
      tauRglob=1/(runif(1,0.26,1)^2)
      tauRwalk=1/(runif(1,0.26,1)^2)
      precisionpropRwalk=runif(1,0.5,1)
      tauIA=1/(runif(mydata$nbsurvey,0.26,1)^2)
      tauIP=1/(runif(mydata$nbtrap,0.26,1)^2)
      tauU=1/(runif(mydata$nbabsolute,0.26,1)^2)
      tauIE=1/(runif(mydata$nbcatch,0.26,1)^2)
      
      epsilonRzone=rnorm(mydata$nbyear*mydata$nbzone,0,1)
      epsilonR=rnorm(mydata$nbyear,0,1)
      beta=runif(1,0.01,2)
      logR1=runif(1,14,17)
      logq=runif(ncol(mydata$logIAObs),-13,0)
      a=apply(mydata$scale_trap,2,function(x) rbeta(1,x[1],x[2]))
      p=apply(mydata$scale_catch,2,function(x) rbeta(1,x[1],x[2]))
      inits=list(tauIE=tauIE,tauq=tauq,propR=propR,tauRglob=tauRglob,
                 tauIA=tauIA,tauIP=tauIP,tauU=tauU, #precisionpropRwalk=precisionpropRwalk,      
                 epsilonRzone=epsilonRzone,epsilonR=epsilonR,epsilonRcm=epsilonRcm,
                 tauRwalk=tauRwalk,beta=beta,
                 logR1=logR1,logIAObs=logIAObs,logIPObs=logIPObs,logUObs=logUObs,logq=logq,a=a,p=p)
      inits
    }
    gen_init(1)
  }
  save.image(paste("datagerem",CY,".rdata",sep=""))
  dbDisconnect(con_wgeel)
  dbDisconnect(con_ccm)
} else{

}
```
Three independent MCMC chains are run in parallel using JAGS [@plummer2003] through R package runjags [@denwood2016]. Chains were run `r format(sample*thin, scientific=FALSE)` iterations, with a thinning of `r format(thin,scientif=FALSE)` iterations, after an initial burnin period of `r format(burnin, scientific=FALSE)` iterations. Gelman and Rubin diagnostics were used to check model convergence [@gelman1992].

```{r modelRun,echo=FALSE,message=FALSE,warning=FALSE}
if  ((!file.exists(paste("gerem",CY,".rdata",sep=""))) | params$runModel){
  jags_res=run.jags("versionBugs2_1.txt",
                    monitor=c("beta","logq","loga",
                              "logRglobal","Rzone","propR","logp"),
                    data=mydata,n.chains=3,
                    inits=generate_init,
                    burnin=burnin,
                    sample=sample,
                    thin=1,
                    tempdir=FALSE,
                    summarise=FALSE,adapt = 80000,
                    keep.jags.files=FALSE,
                    method="parallel")
  
  jags_res=as.mcmc.list(jags_res)
  jags_mat=as.matrix(jags_res)
  colname=varnames(jags_res)
  save.image(paste("gerem",CY,".rdata",sep=""))
          
  
} else {
  load(paste("gerem",CY,".rdata",sep=""))
}
```


# Results
```{r gelman_beta,include=FALSE}
gelman=gelman.diag(jags_res,multivariate=FALSE)
propconv=sum(gelman$psrf[,1]<=1.05)/nrow(gelman$psrf)
beta=quantile(jags_mat[,"beta"],probs=c(0.025,.5,.975))
```
Gelman R hat statistics was below 1.05 for `r round(propconv*100,digits=1)`% of the parameters, demonstrating a good convergence of the model. 


## Overall recruitment and zone zone recruitments
```{r statsRglobal}
rCy=jags_mat[,paste("logRglobal[",mydata$nbyear,"]",sep="")] #last year estimate
rRef=rowMeans(jags_mat[,paste("logRglobal[",1:match("1979",rownames(values_wide)),"]",sep="")])
rstdCY=round(quantile(exp(rCy)/exp(rRef),probs=c(0.025,0.5,0.975))*100,digits=2)
```
Unsurprisingly, overall recruitment (Figure \@ref(fig:rglobal)) shows a steep decline since the early 1980s, despite some oscillations. In the recent period, a period of increase in the early 2010s but seems to stabilize or slightly decrease after this. Credibility interval are rather large at the end of the period both because 2020 were not available when the model was run, and because many time series (especially French fishery based time series) ended after the implementation of the Eel Regulation. The 2020 recruitment is estimated to be `r rstdCY[2]`% (credibility interval [`r paste(rstdCY[c(1,3)],collapse="%-")`%]). 

```{r rglobal, fig.cap="Overall trend in recruitment: median of the posterior distribution (solid line) and corresponding 95% credibility interval (shaded area)",echo=FALSE}
col=grep("logRglobal",colname)
year=sapply(strsplit(colname[col],"[[:punct:]]"),function(x) as.numeric(row.names(values_wide)[as.numeric(x[2])]))
logrglobal=data.frame(t(apply(jags_mat[,col],2,quantile,probs=c(0.025,.5,.975))))
names(logrglobal)=c("q2.5","q50","q97.5")
logrglobal=rbind.data.frame(logrglobal-log(1e3),exp(logrglobal)/1e3)

logrglobal$year=c(year,year)
logrglobal$type=c(rep("log R",length(year)),rep("R",length(year)))

mylabels <- c(`R`="R[y]~~~plain((t))",`log R`="log(R[y])~~~plain((log (t)))")
new.labs <- as_labeller(mylabels, label_parsed)

ggplot(logrglobal,aes(x=year,y=q50))+
  geom_line()+
  geom_ribbon(aes(ymin=q2.5,ymax=q97.5),alpha=0.3)+
  facet_wrap(~type,scales="free",
             labeller=new.labs)+
  theme_bw()+
  ylab("")+
  xlab("")

```

At the zone level (Figure \@ref(fig:rzone)), all zones display a decrease of recruitment. Results confirm that the decline in North Sea started earlier than ATL_F and ATL_IB. The Mediterreanean area also displays a decline in the 1960s, however, estimates in this period are based on few fishery-based time series and the assumption about constant exploitation rate and reporting rate is questionnable, moreover, it is worthwile mentionning that there are currently only 4 time-series are available while the zone is large and include both lagoons and “usual” river basins. For the Channel, the lack of data in the begginning of the time series explain the large credibility interval at the beginning of period, therefore estimates should be taken with great care. ATL_F does not display any increase at the end of the time-series, however, results are based on single time-series (GiscG) and, consequently, confidence interval are rather large.

```{r rzone,echo=FALSE, fig.cap="Trend in recruitment in each zone of the model: median of the posterior distribution (solid line) and corresponding 95% credibility interval (shaded area). The colour of the points on the x axis indicate the number of available data series for the corresponding zone and year"}
col=grep("Rzone",colname)
name_zone=sapply(strsplit(colname[col],"[[:punct:]]"),function(x) zone$zone[as.numeric(x[3])])
year=sapply(strsplit(colname[col],"[[:punct:]]"),function(x) as.numeric(row.names(values_wide)[as.numeric(x[2])]))
rzone=data.frame(t(apply(log(jags_mat[,col]/1e3),2,quantile,probs=c(0.025,.5,.975))))
names(rzone)=c("q2.5","q50","q97.5")
rzone$year=year
rzone$zone=name_zone

ggplot(rzone,aes(x=year,y=q50))+
  geom_line()+
  geom_point(data=data_points,aes(x=das_year,y=0,pch=pre,col=n),cex=.8)+
  geom_ribbon(aes(ymin=q2.5,ymax=q97.5),alpha=0.3)+
  facet_wrap(~zone)+
  ylab(expression(log(R[z])~~~~~(log(t))))+
  scale_colour_viridis_c("number of\ndata points")+
  guides(pch=FALSE)+
  theme_bw()
```

It is also possible to analyse the proportions of recruitment arriving in each zone of the model (Figure \@ref(fig:propZone)). However, these results should be taken with great care: creadibility intervals are large and some zones estimates are based on few absolute (or trap/commercial catch) time series. The decrease in ATL_F is due to the absence of detected increase at the end of the time-series, but this result is based on a single time series and therefore should be taken with care.



```{r propZone, fig.caption="Poportions of overall recruitment arriving in each zone: : median of the posterior distribution (solid line) and corresponding 95% credibility interval (shaded area)", echo=FALSE }
col=grep("propR",colname)
name_zone=sapply(strsplit(colname[col],"[[:punct:]]"),function(x) zone$zone[as.numeric(x[2])])
year=sapply(strsplit(colname[col],"[[:punct:]]"),function(x) as.numeric(row.names(values_wide)[as.numeric(x[3])]))
propR=data.frame(t(apply(jags_mat[,col],2,quantile,probs=c(0.025,.5,.975))))
names(propR)=c("q2.5","q50","q97.5")
propR$year=year
propR$zone=name_zone


ggplot(propR,aes(x=year,y=q50))+
  geom_line()+
  geom_point(data=data_points,aes(x=das_year,y=0,pch=pre,col=n),cex=.8)+
  geom_ribbon(aes(ymin=q2.5,ymax=q97.5),alpha=0.3)+
  facet_wrap(~zone)+
  ylab(expression(log(R[z])~~~~~(log(t))))+
  scale_colour_viridis_c("number of\ndata points")+
  guides(pch=FALSE)+
  theme_bw()
```
## Model fits to observations
Figures \@ref(fig:rbasintrap), \@ref(fig:rbasinabsolute), \@ref(fig:rbasinsurvey) and \@ref(fig:rbasincatch) shows how the model fits observations. In most situations, the model appropriately mimicks the trends of the time series. However, some understimations (e. g. Somme river) or overestimations (ImsaGY) are observed for some series, because absolute estimates are a tradeoff between all time series. Some details on each time series and possible explaination for those biases are detailed in the supporting information of @bornarel2018a. This leads to two conclusions regarding the results:

* trends seem well estimated provided that data are available (see the credibility intervals in Figure 3.2 as soon as data are missing) 
* absolutes estimates should be taken with caution and it was demonstrated that they are sensitive to some parameters of the model [@bornarel2018a].


```{r rbasintrap, echo=FALSE,fig.cap="Model fits to trap time series. The blue lines indicates the median of the posterior distribution and the blue ribbon the corresponding 95% credibility interval, the red points indicate observation"}
trap_long=pivot_longer(cbind.data.frame(trap,data.frame(year=as.numeric(rownames(trap)))),
                        cols=-year,names_to="ser_name_short",values_to="obs")
trap_long$itrap=match(trap_long$ser_name_short,colnames(trap))

trap_pred=do.call(rbind.data.frame,lapply(1:ncol(trap),function(i){
  ser_name_short=names(trap)[i]
  cm=mydata$catchment_trap[i]
  Rcm=jags_mat[,grep(paste("Rcm\\[[0-9]*,",cm,"\\]",sep=""),colname)]
  if (mydata$nbtrap>1){
    scale=jags_mat[,paste("loga[",i,"]",sep="")]
  } else{
    scale=jags_mat[,"loga"]
  }
  tau=-0.5/jags_mat[,paste("tauIP[",i,"]",sep="")]
  
  logRpred=data.frame(t(apply(sweep(log(Rcm),1,scale+tau,"+"),2,quantile,probs=c(0.025,.5,.975))))
  names(logRpred)=c("q2.5","q50","q97.5")
  logRpred$ser_name_short=ser_name_short
  logRpred$year=as.integer(rownames(trap))
  logRpred
}))
trap_obs_pred=merge(trap_long,trap_pred)
print(ggplot(trap_obs_pred,aes(x=year))+
  geom_point(aes(y=log(obs)),col=2)+
  geom_line(aes(y=q50),col="blue")+
  geom_ribbon(aes(ymin=q2.5,ymax=q97.5),alpha=.3,fill="blue")+
  facet_wrap(~ser_name_short)+
  ylab("observed abundance (log scale)")+
  theme_bw())


```

```{r rbasinabsolute,echo=FALSE,fig.cap="Model fits to absolute time series. The blue lines indicates the median of the posterior distribution and the blue ribbon the corresponding 95% credibility interval, the red points indicate observation"}
absolute_long=pivot_longer(cbind.data.frame(absolute,data.frame(year=as.numeric(rownames(absolute)))),
                        cols=-year,names_to="ser_name_short",values_to="obs")
absolute_long$iabsolute=match(absolute_long$ser_name_short,colnames(absolute))

absolute_pred=do.call(rbind.data.frame,lapply(1:ncol(absolute),function(i){
  ser_name_short=names(absolute)[i]
  cm=mydata$catchment_absolute[i]
  Rcm=jags_mat[,grep(paste("Rcm\\[[0-9]*,",cm,"\\]",sep=""),colname)]
  tau=-0.5/jags_mat[,paste("tauU[",i,"]",sep="")]
  
  logRpred=data.frame(t(apply(sweep(log(Rcm),1,tau,"+"),2,quantile,probs=c(0.025,.5,.975))))
  names(logRpred)=c("q2.5","q50","q97.5")
  logRpred$ser_name_short=ser_name_short
  logRpred$year=as.integer(rownames(absolute))
  logRpred
}))
absolute_obs_pred=merge(absolute_long,absolute_pred)
ggplot(absolute_obs_pred,aes(x=year))+
  geom_point(aes(y=log(obs)),col=2)+
  geom_line(aes(y=q50),col="blue")+
  geom_ribbon(aes(ymin=q2.5,ymax=q97.5),alpha=.3,fill="blue")+
  facet_wrap(~ser_name_short)+
  ylab("observed abundance (log scale)")+
  theme_bw()


```


```{r rbasincatch,echo=FALSE, fig.cap="Model fits to the Somme series. The blue lines indicates the median of the posterior distribution and the blue ribbon the corresponding 95% credibility interval, the red points indicate observation"}
catch_long=pivot_longer(cbind.data.frame(catch,data.frame(year=as.numeric(rownames(catch)))),
                        cols=-year,names_to="ser_name_short",values_to="obs")
catch_long$icatch=match(catch_long$ser_name_short,colnames(catch))

catch_pred=do.call(rbind.data.frame,lapply(1:ncol(catch),function(i){
  ser_name_short=names(catch)[i]
  cm=mydata$catchment_catch[i]
  Rcm=jags_mat[,grep(paste("Rcm\\[[0-9]*,",cm,"\\]",sep=""),colname)]
  if (mydata$nbcatch>1){
    scale=jags_mat[,paste("logp[",i,"]",sep="")]
  } else{
    scale=jags_mat[,"logp"]
  }
  if (mydata$nbcatch>1){
    tau=-0.5/jags_mat[,paste("tauIE[",i,"]",sep="")]
  } else{
    tau=-0.5/jags_mat[,"tauIE"]
  }
  
  
  logRpred=data.frame(t(apply(sweep(log(Rcm),1,scale+tau,"+"),2,quantile,probs=c(0.025,.5,.975))))
  names(logRpred)=c("q2.5","q50","q97.5")
  logRpred$ser_name_short=ser_name_short
  logRpred$year=as.integer(rownames(catch))
  logRpred
}))
catch_obs_pred=merge(catch_long,catch_pred)
ggplot(catch_obs_pred,aes(x=year))+
  geom_point(aes(y=log(obs)),col=2)+
  geom_line(aes(y=q50),col="blue")+
  geom_ribbon(aes(ymin=q2.5,ymax=q97.5),alpha=.3,fill="blue")+
  facet_wrap(~ser_name_short)+
  ylab("observed abundance (log scale)")+
  theme_bw()


```

```{r rbasinsurvey, echo=FALSE,fig.cap="Model fits to relative time series. The blue lines indicates the median of the posterior distribution and the blue ribbon the corresponding 95% credibility interval, the red points indicate observation"}
survey_long=pivot_longer(cbind.data.frame(serie,data.frame(year=as.numeric(rownames(serie)))),
                        cols=-year,names_to="ser_name_short",values_to="obs")
survey_long$isurvey=match(survey_long$ser_name_short,colnames(serie))

survey_pred=do.call(rbind.data.frame,lapply(1:ncol(serie),function(i){
  ser_name_short=names(serie)[i]
  cm=mydata$catchment_survey[i]
  Rcm=jags_mat[,grep(paste("Rcm\\[[0-9]*,",cm,"\\]",sep=""),colname)]
  if (mydata$nbsurvey>1){
    scale=jags_mat[,paste("logq[",i,"]",sep="")]
  } else{
    scale=jags_mat[,"logq"]
  }
  tau=-0.5/jags_mat[,paste("tauIA[",i,"]",sep="")]
  
  logRpred=data.frame(t(apply(sweep(log(Rcm),1,tau+scale,"+"),2,quantile,probs=c(0.025,.5,.975))))
  names(logRpred)=c("q2.5","q50","q97.5")
  logRpred$ser_name_short=ser_name_short
  logRpred$year=as.integer(rownames(serie))
  logRpred
}))
survey_obs_pred=merge(survey_long,survey_pred)
ggplot(survey_obs_pred,aes(x=year))+
  geom_point(aes(y=log(obs)),col=2)+
  geom_line(aes(y=q50),col="blue")+
  geom_ribbon(aes(ymin=q2.5,ymax=q97.5),alpha=.3,fill="blue")+
  facet_wrap(~ser_name_short)+
  ylab("observed abundance (log scale)")+
  theme_bw()


```


# Discussion
The use of GEREM does not change the overall image of the recruitment as provided by the GLM analysis. It confirms the decline of recruitment since the 1980s and the currently very low level of recruitment. However, it raises additional questions regarding some potential differences in trends among zones, such as the recent decline in the recruitment received in ATL_F. While definitive conclusions can not be drawn, this result shows the importance of establishing new monitoring time series in areas where data are missing. As such, the monitoring networing implemented in Sudoang appears to be an interesting opportunity. Regarding absolute recruitments, as already mentioned, results should be taken with great care since the number of time series is limited, the estimates are sensitive to some parameters and biases are observed in the model fits. However, achieving absolute estimates would be required to compare the importance of recruitment among zones. Since estimation of escapments was requested by the Eel Regulation, the parallel estimation of absolute recruitment would allow comparison of survival rates during the continental stage among zones. As such a better understing on how local recruitment in river basins depends on local characteristics (basin surface and potentially other factors) would be a valuable information for management of standing stocks and would subsequently allow to improve the model. This calls for achieving more absolute recruitment estimates.


# Conclusion
The idea of presenting this modelling exercice was not to replace the GLM exercice nor to conduct a benchmark exercice of model but to provide an additional tool that provides complementary information. The two modelling approaches have two different level of complexity and provide similar general picture of the trend of recruitment. While GEREM does not provide any definitive conclusions, it raises interesting complementary questions and highlights the need of new data in some regions and of new types.


#References